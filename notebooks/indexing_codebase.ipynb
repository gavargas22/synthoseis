{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the codebase of synthoseis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "# os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "# os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass('Activeloop Token:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, OpenAI, Cohere, HuggingFaceHub\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('C:\\\\Users\\\\gavargas\\\\Developer\\\\synthoseis\\\\datagenerator')\n",
    "# loader = TextLoader(root_dir)\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "TEXT_SPLITTER_CHUNK_PARAMS = {\n",
    "    \"chunk_size\": 2000,\n",
    "    \"chunk_overlap\": 0,\n",
    "    \"length_function\": len,\n",
    "}\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(**TEXT_SPLITTER_CHUNK_PARAMS)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gavargas\\Developer\\synthoseis\\notebooks\\indexing_codebase.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gavargas/Developer/synthoseis/notebooks/indexing_codebase.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gavargas/Developer/synthoseis/notebooks/indexing_codebase.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gavargas/Developer/synthoseis/notebooks/indexing_codebase.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m text_vdb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_texts(texts, embeddings)\n",
      "File \u001b[1;32mh:\\Miniconda\\envs\\synthoseis\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:610\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[0;32m    584\u001b[0m \u001b[39mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    602\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    603\u001b[0m     embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    609\u001b[0m )\n\u001b[1;32m--> 610\u001b[0m chroma_collection\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[0;32m    611\u001b[0m \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mh:\\Miniconda\\envs\\synthoseis\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:188\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(texts)\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m metadatas:\n\u001b[0;32m    190\u001b[0m     \u001b[39m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[39m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     length_diff \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(texts) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mh:\\Miniconda\\envs\\synthoseis\\Lib\\site-packages\\langchain\\embeddings\\openai.py:490\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32mh:\\Miniconda\\envs\\synthoseis\\Lib\\site-packages\\langchain\\embeddings\\openai.py:351\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    348\u001b[0m     \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 351\u001b[0m token \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39;49mencode(\n\u001b[0;32m    352\u001b[0m     text,\n\u001b[0;32m    353\u001b[0m     allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallowed_special,\n\u001b[0;32m    354\u001b[0m     disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisallowed_special,\n\u001b[0;32m    355\u001b[0m )\n\u001b[0;32m    356\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(token), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length):\n\u001b[0;32m    357\u001b[0m     tokens\u001b[39m.\u001b[39mappend(token[j : j \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length])\n",
      "File \u001b[1;32mh:\\Miniconda\\envs\\synthoseis\\Lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(disallowed_special, \u001b[39mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39;49msearch(text):\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "text_vdb = Chroma.from_texts(texts, embeddings, metadatas=metadatas, dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthoseis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
